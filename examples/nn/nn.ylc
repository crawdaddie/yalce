import ../../lib/Math;
import ../../lib/Arrays;
import dataset;


let (@) = array_at;
let print_arr = fn arr: (Array of Double) -> 
  print "[| ";
  for i = 0 to (array_size arr) in (
    print `{arr @ i}, `
  );
  print "|]\n"
  
;;


type Matrix = (
  rows: Int,
  cols: Int,
  data: Array of Double
);

let print_matrix = fn m: (Matrix) ->
  for i = 0 to m.rows in (
    for j = 0 to m.cols in (
      print `{m.data @ (i * m.cols + j)}, `
    );
    print "\n"
  )
;;

type Layer = (
  weights: Matrix,
  biases: Array of Double,
  activation: ((Array of Double) -> (Array of Double))
);

let matrix_zeroes = fn c r ->
  Matrix r c (array_fill_const (r * c) 0.)
;;

let matrix_to_str = fn m: (Matrix) ->
  ""
;;


let relu = fn x: (Array of Double) -> 
  for i = 0 to (array_size x) in (
    let v = x @ i in
    array_set i x (match v with
    | v if v > 0. -> v
    | _ -> 0.)
  );
  x 
;;

# let relu_derivative = fn x: (Array of Double) -> 
#   for i = 0 to (array_size x) in (
#     let v = x @ i in
#     array_set i x (match v with
#     | v if v > 0. -> v
#     | _ -> 0.)
#   );
#   x 
# ;;

let identity = fn x: (Array of Double) -> x;;

let mse_loss = fn predictions: (Array of Double) targets: (Array of Double) ->
  let loop = fn sum: (Double) p: (Array of Double) t: (Array of Double) ->
    match (array_size p) with
    | 0 -> sum
    | _ -> (

      let pv = p @ 0;
      let tv = t @ 0;
      let loss = Math.pow (pv - tv) 2.;
      loop (sum + loss) (array_succ p) (array_succ t)
    )
  ;;
  let sum = loop 0. predictions targets;
  sum / (array_size predictions)
;;


let matrix_random = fn c r ->
  Matrix r c (array_fill (r * c) (fn i: (Int) -> Math.rand_double_range -1. 1.))
;;



let max_in = array_fill_const 16 0.; # allocate an input array which is as large as the maximum layer width

type Network = (
  layers: Array of Layer,
  loss: ((Array of Double) -> (Array of Double) -> Double)
);

let MLP = Network [|
  Layer (matrix_random 2 16) (array_fill_const 16 0.01) relu,
  Layer (matrix_random 16 16) (array_fill_const 16 0.01) relu,
  Layer (matrix_random 16 1) (array_fill_const 1 1.01) identity,
  |] mse_loss
;

let LayerCache = module

  type LCache = (
    pre_activations: (Array of Array of Double),
    activations: (Array of Array of Double)
  );

  let from_network = fn network: (Network) ->
    let num_layers = array_size network.layers;
    let pre_act_sizes = array_fill_const num_layers 0;
    let act_sizes = array_fill_const (num_layers + 1) 0;

    let layer = network.layers @ 0;
    let weights = layer.weights;
    let cols = weights.cols;
    array_set 0 act_sizes cols;
    let first_act_size = cols;


    let pre_acts = array_fill_const num_layers [| 0. |];
    let acts = array_fill_const (num_layers + 1) [| 0. |];

    array_set 0 acts (array_fill_const cols 0.);

    let ir = [| 0 |];
    for layer = iter network.layers in (
      let i = ir @ 0;
      let pre_size = array_size (layer.biases);
      let act_size = array_size (layer.biases);
      array_set (i + 1) acts (array_fill_const act_size 0.); 
      array_set i pre_acts (array_fill_const pre_size 0.); 

      array_set 0 ir (i + 1)
    );
    LCache pre_acts acts
  ;;

;


let forward_cache = LayerCache.from_network MLP;


let Vec = module
  let _matrix_vec_mul = extern fn Int -> Int -> Ptr of Double -> Ptr of Double -> ();
  let matrix_vec_mul = fn m: (Matrix) v: (Array of Double) ->
    _matrix_vec_mul m.rows m.cols (cstr m.data) (cstr v);
    v
  ;;

  let _vec_add = extern fn Int -> Ptr of Double -> Ptr of Double -> ();

  let add = fn a: (Array of Double) b: (Array of Double) -> 
    _vec_add (array_size a) (cstr a) (cstr b);
    b
  ;;

  let sub = fn a: (Array of Double) b: (Array of Double) ->
    for i = 0 to (array_size a) in (
      let av = a @ i;
      let bv = b @ i;
      array_set i b (av - bv)
    );
    b
  ;;

  let copy = fn a: (Array of Double) ->

    let b = array_fill_const (array_size a) 0.;

    for i = 0 to (array_size b) in (
      let av = a @ i;
      array_set i b av
    );
    b
  ;;

  let write_to = fn to_: (Array of Double) from: (Array of Double) ->
    Arrays.fold (fn t v ->
      array_set 0 t v;
      array_succ t
    ) to_ from;
    from
  ;;
;



let forward = fn network: (Network) cache: (LayerCache.LCache) input ->
  let layers = network.layers;

  input |> Vec.write_to (cache.pre_activations @ 0);

  # TODO: I have to specify the structural types for now - should be possible to fix one day
  Arrays.foldi (fn i (current, cache): (Array of Double, LayerCache.LCache) layer: (Layer) ->
    let z = current
      |> Vec.matrix_vec_mul layer.weights
      |> Vec.add layer.biases
      |> Vec.write_to (cache.pre_activations @ i)
      |> layer.activation
      |> Vec.write_to (cache.activations @ (i + 1))
      ;

    (z, cache)

  ) (input, cache) layers
;;

let (out, cache) = max_in |> forward MLP forward_cache;
print `{out @ 0}\n`;
print `forward_cache {forward_cache.pre_activations @ 0 @ 4}\n`;

let backward = fn network: (Network) cache: (LayerCache.LCache) x y delta ->
  let layers = network.layers; 
  let num_layers = array_size layers;
  
  Vec.write_to delta y;
  let delta = Vec.sub (Arrays.last cache.activations) delta;


  for i_ = 0 to (num_layers) in (
    let i = num_layers - 1 - i_; # reverse i
    let dW = vec_dot (transpose (cache.activations @ i)) delta;
    let db = vec_sum delta;

    print `rev: {i}, \n`
  ) 

;;

backward MLP forward_cache out [|1.|] (array_fill_const 16 0.);

# let train = fn
#   network: (Network)
#   epochs
#   learning_rate ->  
#   for epoch = 0 to epochs in (
#     let loss = 0.01;
#     match epoch % 10 with
#     | 0 -> (print `Epoch {epoch} Loss: {loss}\n`; ())
#     | _ -> ()
#   )
# ;;

# train MLP 1000 0.01 


# print_matrix (matrix_random 16 16);

# print `{(array_range 1 dforward_cache_storage) @ 0}\n`;
