import ../../lib/Math;
open ../../lib/Arrays;
import dataset;
open linalg;

type Layer = (
  weights: Matrix,
  biases: Matrix,
  activation: (Matrix -> Matrix)
);

let identity = fn x: (Matrix) -> x;;

let relu = fn x: (Matrix) -> 
  for i = 0 to (matrix_size x) in (

    let v = x.data[i] in

    x.data[i] := (match v with
      | v if v > 0. -> v
      | _ -> 0.
    )
  );
  x 
;;

let mse_loss = fn predictions: (Matrix) targets: (Matrix) ->
  let loop = fn sum: (Double) p: (Array of Double) t: (Array of Double) ->
    match (array_size p) with
    | 0 -> sum
    | _ -> (

      let pv = p[0];
      let tv = t[0];
      let loss = Math.pow (pv - tv) 2.;
      loop (sum + loss) (array_succ p) (array_succ t)
    )
  ;;
  let sum = loop 0. predictions.data targets.data;
  sum / (matrix_size predictions)
;;

type Network = (
  layers: Array of Layer,
  loss: (Matrix -> Matrix -> Double)
);


let LayerCache = module

  type LCache = (
    pre_activations: Array of Array of Double,
    activations: Array of Array of Double 
  );

  let from_network = fn network: (Network) ->
    let num_layers = array_size network.layers;
    let pre_act_sizes = array_fill_const num_layers 0;
    let act_sizes = array_fill_const (num_layers + 1) 0;

    let pre_acts = array_fill_const num_layers [| 0. |];
    let acts = array_fill_const (num_layers + 1) [| 0. |];
    let layer = network.layers[0];
    let weights = layer.weights;
    acts[0] := (array_fill_const weights.cols 0.) ;

    for i = 0 to num_layers in (
      let layer = network.layers[i];

      let pre_size = matrix_size (layer.biases);
      let act_size = matrix_size (layer.biases);
      acts[i + 1] := (array_fill_const act_size 0.); 
      pre_acts[i] := (array_fill_const pre_size 0.)
      
    );

    LCache pre_acts acts

  ;;
  let print_cache_shapes = fn cache: (LCache) ->
    print `cache sizes: {array_size cache.pre_activations} {array_size cache.activations}\n`;
    print "pre: ";

    for p = (iter (cache.pre_activations)) in (
      print `{array_size p},`
    );
    print `\n`;

    print "acts: ";
    for p = (iter cache.activations) in (
      print `{array_size p},`
    )
  ;;

  let save = fn d: (Array of Double) m: (Matrix) ->
    let raw = m.data;
    for i = 0 to (array_size raw) in (
       d[i] := raw[i]
    );

    m
  ;;
;


let MLP = Network [|
  Layer (matrix_random 2 16) (array_fill_const 16 0.01 |> matrix_1d) relu,
  Layer (matrix_random 16 16) (array_fill_const 16 0.01 |> matrix_1d) relu,
  Layer (matrix_random 16 1) (array_fill_const 1 1.01 |> matrix_1d) identity,
|] mse_loss
;




let forward = fn network: (Network) cache: (LayerCache.LCache) input ->
  let layers = network.layers;
  LayerCache.save (cache.pre_activations[0]) input;

  foldi (fn i (current, cache): (Matrix, LayerCache.LCache) layer: (Layer) ->
    let z = current
      |> Vec.matrix_vec_mul layer.weights
      |> Vec.add layer.biases
      |> LayerCache.save (cache.pre_activations[i]) 
      |> layer.activation
      |> LayerCache.save (cache.activations[i + 1])
    ;
    (z, cache)
  ) (input, cache) layers
;;


let backward = fn learning_rate network: (Network) cache: (LayerCache.LCache) y ->
  let last_activations = last cache.activations;

  let delta = Vec.sub (Matrix 1 16 last_activations) y;

  print `0: backward delta: {delta.rows} {delta.cols}\n`;
  let len = array_size network.layers in
  for _i = 0 to len in (
    let i = len - 1 - _i; # reverse
    let act_array = cache.activations[i];
    let activation = Matrix (array_size act_array) 1 act_array; 
    Vec.matrix_vec_mul activation delta;
    print_matrix delta;

    match i > 0 with
    | true -> ()
    | false -> ()

    # print_matrix activation
    # let dW = dot activation delta
  )
;;

# XOR problem
let X = Matrix 4 2 [|
  0., 0.,
  0., 1.,
  1., 0.,
  1., 1.
|];

let y = Matrix 4 2 [| 0., 1., 1., 0. |];

# let input = matrix_zeroes 16 1; # allocate an input array which is as large as the maximum layer width
let (out, cache) = X |> forward MLP (LayerCache.from_network MLP);

print `out: {matrix_at out (0,0)}\n`;

backward 0.01 MLP cache y




