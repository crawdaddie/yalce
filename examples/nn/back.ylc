let backward = fn learning_rate network: (Network) cache: (LayerCache.LCache) y ->
  let last_activations = last cache.activations;

  # Initial error calculation (prediction - target)
  let delta = Vec.sub (Matrix 1 16 last_activations) y;

  print `0: backward delta: {delta.rows} {delta.cols}\n`;
  let len = array_size network.layers in
  for _i = 0 to len in (
    let i = len - 1 - _i; # reverse
    let layer = network.layers[i];
    let act_array = cache.activations[i];
    let activation = Matrix (array_size act_array) 1 act_array; 
    
    # Calculate weight gradients: dW = a_{l-1}^T @ delta_l
    let dW = Vec.mat_transpose_mul activation delta;
    print `Layer {i} dW: {dW.rows} {dW.cols}\n`;
    
    # Calculate bias gradients: db = sum(delta_l)
    let db = Vec.sum_rows delta;
    print `Layer {i} db: {db.rows} {db.cols}\n`;
    
    # Update weights and biases
    layer.weights := Vec.sub layer.weights (Vec.scalar_mul learning_rate dW);
    layer.biases := Vec.sub layer.biases (Vec.scalar_mul learning_rate db);
    
    # Compute delta for previous layer (if not at first layer)
    match i > 0 with
    | false -> ()
    | true -> (
        # Get activation derivative based on the activation function
        let prev_layer = network.layers[i-1];
        let prev_pre_act = cache.pre_activations[i-1];
        let prev_pre_act_matrix = Matrix (array_size prev_pre_act) 1 prev_pre_act;
        
        let activation_derivative = match prev_layer.activation with
        | f if f == relu -> (
            # ReLU derivative: 1 if x > 0, 0 otherwise
            let derivative = matrix_copy prev_pre_act_matrix;
            for j = 0 to (matrix_size derivative) in (
              derivative.data[j] := (match derivative.data[j] > 0. with
                | true -> 1.
                | false -> 0.
              )
            );
            derivative
          )
        | _ -> (
            # Identity derivative: all ones
            matrix_ones (matrix_size prev_pre_act_matrix) 1
          )
        ;
        
        # Compute delta for previous layer
        # delta = (delta @ W^T) * activation_derivative(prev_pre_act)
        let weights_T = Vec.transpose layer.weights;
        let delta_weight = Vec.matrix_vec_mul delta weights_T;
        delta := Vec.hadamard_mul delta_weight activation_derivative;
        
        print `{i-1}: backward delta: {delta.rows} {delta.cols}\n`;
    )
  );
  
  # Return the last calculated delta (could be used for loss calculation)
  delta
;;
