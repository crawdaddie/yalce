
import ../../lib/Math;
let open_file = extern fn String -> String -> Option of Ptr;
let fclose = extern fn Ptr -> ();
let read_bytes = extern fn Ptr -> String; let read_lines = extern fn Ptr -> (List of String, Int);
let char_to_int = extern fn Char -> Int;


let get_lines = fn () ->
  let (c, l) = match (open_file "examples/micrograd/names.txt" "rb") with
  | Some fd -> (
    let c, l = read_lines fd;
    fclose fd;
    (c, l)
  )
  | None -> ([], 0)
  ;
  (c, l)
;;

let lines, num = get_lines ();

let string_concat = extern fn Ptr -> Int -> String;
let start_char = '^';
let end_char = '$';

let bigrams = fn word: (String) ->
  let chars = `^{word}^`;
  let bg = array_fill_const ((array_size chars) - 1) ('\0', '\0');
  for i = 0 to (array_size chars - 1) in (
    let a, b = (chars[i], chars[i + 1]);
    # print `{i}: {a}:{b}\n`;
    bg[i] := (a, b)
  );
  bg
;;

open Matrix;


let char_to_int = extern fn Char -> Int;
let char_encode = fn ch ->
  match ch with
  | '^' -> 0 
  # | '$' -> 27
  | _ -> (char_to_int ch) - 97 + 1
;;


let log = extern fn Double -> Double;
let num_bigrams_ = [| 0 |];
for word = iter lines in (
  for bgs = iter (bigrams word) in (
    num_bigrams_[0] := 1 + num_bigrams_[0]
  ) 
);
let num_bigrams = num_bigrams_[0];
print `num bigrams {num_bigrams}\n`;



let one_hot = fn i n -> 
  let arr = array_fill_const n 0;
  arr[i] := 1;
  arr
;;


let xencs = Matrix num_bigrams 27 @@ array_fill_const (27 * num_bigrams) 0.;
let yencs = Matrix num_bigrams 27 @@ array_fill_const (27 * num_bigrams) 0.;
let xs = array_fill_const num_bigrams 0;
let ys = array_fill_const num_bigrams 0;

# convert each bigram to one-hot encodings
let bg_idx = [| 0 |];
for word = iter lines in (
  for bgs = iter (bigrams word) in (
    let idx = bg_idx[0];
    let a, b = bgs;
    let i = char_encode a;
    let j = char_encode b;

    xencs.data[idx * 27 + i] := 1.; 
    yencs.data[idx * 27 + j] := 1.; 
    xs[idx] := i;
    ys[idx] := j;

    bg_idx[0] := idx + 1
  ) 
);
let softmax_normalize = fn m: (Matrix of Double) ->
  let result = Matrix m.rows m.cols (Storage.arr_alloc (m.rows * m.cols));
  for i = 0 to m.rows in (
    # // Numerical stability: subtract max
    let max_val = [| m.data[i * m.cols] |];
    for j = 1 to m.cols in (
      let val = m.data[i * m.cols + j];
      max_val[0] := Math.max val (max_val[0])
    );
    
    # // Compute exp and sum
    let sum_exp = [| 0. |];
    for j = 0 to m.cols in (
      let exp_val = Math.exp ((mget m (i, j)) - max_val[0]);
      mset result (i, j) exp_val;
      sum_exp[0] := sum_exp[0] + exp_val
    );
    
    # // Normalize
    for j = 0 to m.cols in (
      let idx = i * m.cols + j;
      result.data[idx] := result.data[idx] / sum_exp[0]
    )
  );
  result
;;

let __forward = fn x: (Matrix of Double) w: (Matrix of Double) ->

  # print "get probs\n";
  let probs = 
    mmul_tmp x w # log-counts
    |> mexp  # - counts equivalent to bigram_freqs in ./makemore_simple_markov.ylc
    |> mrow_l2_norm; # counts normalized row-wise equivalent to bigram_dists in ./makemore_simple_markov.ylc

  let nlls = [| 0. |];
  let nrows = x.rows;
  # print "get losses\n";

  for i = 0 to nrows in (
    let x = xs[i]; # i-th bigram
    let y = ys[i]; # i-th bigram

    let nll = mget probs (i, y)
      |> log
      |> (*) -1.
      |> (/) nrows;

    nlls[0] := nlls[0] + nll
  );
  (probs, nlls[0])
;;
# // 1. One-hot encode input characters
let forward = fn x: (Matrix of Double) w: (Matrix of Double) ->
  # x is one-hot encoded: each row has one 1.0 and rest 0.0
  # w is weight matrix [27 x 27]

  let probs = 
    mmul_tmp x w # log-counts
    |> mexp  # - counts equivalent to bigram_freqs in ./makemore_simple_markov.ylc
    |> mrow_l2_norm; # counts normalized row-wise equivalent to bigram_dists in ./makemore_simple_markov.ylc
  
  # // 3. Cross-entropy loss
  let loss = [| 0. |];
  for i = 0 to x.rows in (

    let nll = (i, ys[i])
      |> mget probs
      |> log
      |> (*) -1.;

    loss[0] := loss[0] + nll
  );
  loss[0] := loss[0] / x.rows;  #// Average loss
  
  (probs, loss[0])
;;

let backward = fn learning_rate: (Double) x: (Matrix of Double) w: (Matrix of Double) probs: (Matrix of Double) ->
  # Create gradient matrix: dL/dlogits = probs - targets
  let dlogits = Matrix probs.rows probs.cols (Storage.arr_alloc (probs.rows * probs.cols));
  
  # Copy probabilities
  for i = 0 to (msize probs) in (
    dlogits.data[i] := probs.data[i]
  );
  
   # Subtract 1.0 from correct class positions  
  for i = 0 to probs.rows in (
    let correct_class = ys[i];
    let idx = i * probs.cols + correct_class;
    dlogits.data[idx] := dlogits.data[idx] - 1.0
  );
  
  # Average gradients across batch
  for i = 0 to (msize dlogits) in (
    dlogits.data[i] := dlogits.data[i] / probs.rows
  );
  
  # Compute weight gradients: dW = X^T * dlogits
  let XT = mtrans_tmp x;
  let dW = mmul_tmp XT dlogits;
  
  # Update weights: W -= learning_rate * dW
  for i = 0 to (msize w) in (
    w.data[i] := w.data[i] - (learning_rate * dW.data[i])
  )
;;

let X = xencs;
let W = Matrix 27 27 (array_fill (27 * 27) (fn i: (Int) -> Math.randn 0.1 0.2));

for epoch = 0 to 50 in (
  Storage.reset ();
  let p, l = forward X W;
  print `loss: {l}\n`;
  backward 50. xencs W p
  # ;
  # mprint W
)

