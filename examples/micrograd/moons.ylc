open linalg;
import ../../lib/Math;
open ../../lib/Arrays;

type Layer = (
  weights: Matrix,
  biases: Matrix,
  activation: (Matrix -> Matrix),
  activation_derivative: (Matrix -> Matrix)
);


let relu = fn x: (Matrix) -> 
  for i = 0 to (matrix_size x) in (

    let v = x.data[i] in

    x.data[i] := (match (v > 0.) with
      | true -> v
      | _ -> 0.
    )
  );
  x 
;;

let relu_derivative = fn x: (Matrix) -> 
  let x_ = matrix_zeroes x.rows x.cols;

  for i = 0 to (matrix_size x) in (

    let v = x.data[i] in

    x_.data[i] := (match (v > 0.) with
      | true -> 1.
      | _ -> 0.
    )
  );
  x_ 
;;

let tanh_activation = fn x: (Matrix) -> 
  for i = 0 to (matrix_size x) in (
    let v = x.data[i] in
    x.data[i] := Math.tanh v
  );
  x 
;;

let tanh_derivative = fn x: (Matrix) -> 
  let x_ = matrix_zeroes x.rows x.cols;
  for i = 0 to (matrix_size x) in (
    let v = x.data[i] in
    let tanh_v = Math.tanh v in
    x_.data[i] := 1. - (tanh_v * tanh_v)
  );
  x_ 
;;

let identity = fn x: (Matrix) -> x;;
let identity_derivative = fn x: (Matrix) -> 
  Matrix x.rows x.cols (array_fill_const (x.rows * x.cols) 1.)
;;


let mse_loss = fn predictions: (Matrix) targets: (Matrix) ->
  let loop = fn sum: (Double) p: (Array of Double) t: (Array of Double) ->
    match (array_size p) with
    | 0 -> sum
    | _ -> (

      let pv = p[0];
      let tv = t[0];
      let loss = Math.pow (pv - tv) 2.;
      loop (sum + loss) (array_succ p) (array_succ t)
    )
  ;;
  let sum = loop 0. predictions.data targets.data;
  sum / (matrix_size predictions)
;;

type Network = (
  layers: Array of Layer,
  loss: (Matrix -> Matrix -> Double)
);

let LayerCache = module
  type LCache = (
    pre_activations: Array of Matrix,  
    activations: Array of Matrix       
  );

  let from_network = fn network: (Network) batch_size: (Int) features: (Int)->
    let num_layers = array_size network.layers;
    
    let pre_acts = array_fill_const num_layers (matrix_zeroes 1 1);
    let acts = array_fill_const (num_layers + 1) (matrix_zeroes 1 1);
    
    acts[0] := matrix_zeroes batch_size features; 
    
    for i = 0 to num_layers in (
      let layer = network.layers[i];
      
      let output_dim = matrix_size layer.biases;
      
      pre_acts[i] := matrix_zeroes batch_size output_dim;
      
      acts[i + 1] := matrix_zeroes batch_size output_dim
    );
    
    LCache pre_acts acts
  ;;

  let save = fn cache: (Matrix) target: (Matrix) ->
    for i = 0 to array_size cache.data in (
      cache.data[i] := target.data[i] 
    );
    target
  ;;
;




let forward = fn network: (Network) cache: (LayerCache.LCache) input ->
  let layers = network.layers;
  LayerCache.save (cache.activations[0]) input;

  let res = foldi (fn i (current, cache): (Matrix, LayerCache.LCache) layer: (Layer) ->
    let z = current
      |> matrix_mul layer.weights
      |> matrix_add layer.biases
      |> LayerCache.save (cache.pre_activations[i]) 
      |> layer.activation
      |> LayerCache.save (cache.activations[i + 1])
    ;

    (z, cache)
  ) (input, cache) layers
  ;

  res
;;


let infer = fn network: (Network) input ->
  let layers = network.layers;

  let res = foldi (fn i current: (Matrix) layer: (Layer) ->
    let z = current
      |> matrix_mul layer.weights
      |> matrix_add layer.biases
      |> layer.activation
    ;
    z

  ) input layers
  ;

  res
;;

let backward = fn learning_rate network: (Network) cache: (LayerCache.LCache) y ->
  let last_activations = last cache.activations;
  let d = matrix_sub last_activations y;
  let delta_ref = [|d|];

  let len = array_size network.layers;

  for _i = 0 to len in (
    let i = len - 1 - _i; # reverse

    let layer = network.layers[i];

    let activation = cache.activations[i]; 
    let dW = matrix_mul (delta_ref[0]) (matrix_transpose activation) ;
    let db = matrix_sum_rows (delta_ref[0]);

    matrix_sub layer.weights (matrix_scale learning_rate dW)
    |> matrix_copy_to layer.weights;

    matrix_sub layer.biases (matrix_scale learning_rate db)
    |> matrix_copy_to layer.biases;

    match i > 0 with
    | true -> (
      let pre_activation = cache.pre_activations[i - 1];

      let delta = delta_ref[0]
        |> matrix_mul (matrix_transpose layer.weights)
        |> matrix_mul_elwise (layer.activation_derivative pre_activation);

      delta_ref[0] := delta;
      ()
    )
    | _ -> ()
  )
;;


import dataset;

let df = dataset.load "examples/micrograd/moons.csv";

let MLP = Network [|
  Layer (matrix_random 16 2) (matrix_random_bias 16) relu relu_derivative,
  Layer (matrix_random 16 16) (matrix_random_bias 16) relu relu_derivative,
  Layer (matrix_random 1 16) (matrix_random_bias 1) tanh_activation tanh_derivative,
|] mse_loss
;



let cache = LayerCache.from_network MLP 4 2;

for epoch = 0 to 10000 in (
  
  let loss = [|0.|];
  let learning_rate = match epoch with
      | n if n < 2000 -> 0.01
      | n if n < 5000 -> 0.005
      | n if n < 8000 -> 0.001
      | _ -> 0.0005
  ;
  let shuf = Math.shuffle 80;

  for i = 0 to 20 in (
    pool_reset ();
    let i = 4 * i;
    # let i = shuf[i];

    let batch = Matrix 4 2 [|
      df.x1[i],     df.x2[i],
      df.x1[i + 1], df.x2[i + 1],
      df.x1[i + 2], df.x2[i + 2],
      df.x1[i + 3], df.x2[i + 3],
    |];

    let (out, cache) = batch |> forward MLP cache;
    let y = Matrix 4 1 [|
      Double (df.label[i]),
      Double (df.label[i + 1]),
      Double (df.label[i + 2]),
      Double (df.label[i + 3]),
    |];

    backward learning_rate MLP cache y;

    loss[0] := loss[0] + ((mse_loss out y) / 80.)
  );

  match (epoch % 100) with
  | 0 -> (
    print `epoch {epoch} loss {loss[0]} lr {learning_rate}\n`;
    ()
  )
  | _ -> ()

);

#
let input_test = fn () ->

  let corrects = [|0.|];
  for i = 80 to 100 in (
    let row = Matrix 1 2 [|df.x1[i], df.x2[i]|];
    let expected = Matrix 1 1 [|Double df.label[i]|];
    let out = row |> infer MLP;
    let d = df.label[i] + (matrix_get out (0, 0));
    corrects[0] := corrects[0] + (Math.abs (d / 2));
    print `TEST {i}: {df.x1[i]}, {df.x2[i]} -> {df.label[i]} - got {matrix_get out (0, 0)}\n`
  );
  print `accuracy: {100. * corrects[0] / 20.}%\n`
;;


input_test ()
int char_to_int(char c) {
  return (int)c;
}
