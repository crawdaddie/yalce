import ../../lib/Math;
open ../../lib/Arrays;
import dataset;
open linalg;

type Layer = (
  weights: Matrix,
  biases: Matrix,
  activation: (Matrix -> Matrix),
  activation_derivative: (Matrix -> Matrix)
);


let relu = fn x: (Matrix) -> 
  for i = 0 to (matrix_size x) in (

    let v = x.data[i] in

    x.data[i] := (match (v > 0.) with
      | true -> v
      | _ -> 0.
    )
  );
  x 
;;

let relu_derivative = fn x: (Matrix) -> 
  let x_ = matrix_zeroes x.rows x.cols;

  for i = 0 to (matrix_size x) in (
    let v = x.data[i] in
    x_.data[i] := (match (v > 0.) with
      | true -> 1.
      | _ -> 0.
    )
  );
  x_ 
;;

let identity = fn x: (Matrix) -> x;;
let identity_derivative = fn x: (Matrix) -> 
  Matrix x.rows x.cols (array_fill_const (x.rows * x.cols) 1.)
;;


let mse_loss = fn predictions: (Matrix) targets: (Matrix) ->
  let loop = fn sum: (Double) p: (Array of Double) t: (Array of Double) ->
    match (array_size p) with
    | 0 -> sum
    | _ -> (

      let pv = p[0];
      let tv = t[0];
      let loss = Math.pow (pv - tv) 2.;
      loop (sum + loss) (array_succ p) (array_succ t)
    )
  ;;
  let sum = loop 0. predictions.data targets.data;
  sum / (matrix_size predictions)
;;

type Network = (
  layers: Array of Layer,
  loss: (Matrix -> Matrix -> Double)
);

let LayerCache = module
  type LCache = (
    pre_activations: Array of Matrix,  
    activations: Array of Matrix       
  );

  let from_network = fn network: (Network) batch_size: (Int) features: (Int)->
    let num_layers = array_size network.layers;
    
    let pre_acts = array_fill_const num_layers (matrix_zeroes 1 1);
    let acts = array_fill_const (num_layers + 1) (matrix_zeroes 1 1);
    
    acts[0] := matrix_zeroes batch_size features; 
    
    for i = 0 to num_layers in (
      let layer = network.layers[i];
      
      let output_dim = matrix_size layer.biases;
      
      pre_acts[i] := matrix_zeroes batch_size output_dim;
      
      acts[i + 1] := matrix_zeroes batch_size output_dim
    );
    
    LCache pre_acts acts
  ;;

  let save = fn cache: (Matrix) target: (Matrix) ->
    for i = 0 to array_size cache.data in (
      cache.data[i] := target.data[i] 
    );
    target
  ;;
;


let MLP = Network [|
  Layer (matrix_random 16 2) (matrix_random_bias 16) relu relu_derivative,
  Layer (matrix_random 16 16) (matrix_random_bias 16) relu relu_derivative,
  Layer (matrix_random 1 16) (matrix_random_bias 1) identity identity_derivative,
|] mse_loss
;


let forward = fn network: (Network) cache: (LayerCache.LCache) input ->
  let layers = network.layers;
  LayerCache.save (cache.activations[0]) input;

  let res = foldi (fn i (current, cache): (Matrix, LayerCache.LCache) layer: (Layer) ->
    let z = current
      |> matrix_mul layer.weights
      |> matrix_add layer.biases
      |> LayerCache.save (cache.pre_activations[i]) 
      |> layer.activation
      |> LayerCache.save (cache.activations[i + 1])
    ;

    (z, cache)
  ) (input, cache) layers
  ;

  res
;;


let infer = fn network: (Network) input ->
  let layers = network.layers;

  let res = foldi (fn i current: (Matrix) layer: (Layer) ->
    let z = current
      |> matrix_mul layer.weights
      |> matrix_add layer.biases
      |> layer.activation
    ;
    z

  ) input layers
  ;

  res
;;

let backward = fn learning_rate network: (Network) cache: (LayerCache.LCache) y ->
  let last_activations = last cache.activations;
  let d = matrix_sub last_activations y;
  let delta_ref = [|d|];

  let len = array_size network.layers;

  for _i = 0 to len in (
    let i = len - 1 - _i; # reverse

    let layer = network.layers[i];

    let activation = cache.activations[i]; 
    let dW = matrix_mul (delta_ref[0]) (matrix_transpose activation) ;
    let db = matrix_sum_rows (delta_ref[0]);

    matrix_sub layer.weights (matrix_scale learning_rate dW)
    |> matrix_copy_to layer.weights;

    matrix_sub layer.biases (matrix_scale learning_rate db)
    |> matrix_copy_to layer.biases;

    match i > 0 with
    | true -> (
      let pre_activation = cache.pre_activations[i - 1];

      let delta = delta_ref[0]
        |> matrix_mul (matrix_transpose layer.weights)
        |> matrix_mul_elwise (layer.activation_derivative pre_activation);

      delta_ref[0] := delta;
      ()
    )
    | _ -> ()
  )
;;

# XOR problem
let X = Matrix 4 2 [|
  0., 0.,
  0., 1.,
  1., 0.,
  1., 1.
|];

let Y = Matrix 4 1 [| 0., 1., 1., 0. |];
let cache = LayerCache.from_network MLP 1 2;

for epoch = 0 to 10000 in (
  
  pool_reset ();
  for i = 0 to X.rows in (
    let row = matrix_row X i;
    let (out, cache) = row |> forward MLP cache;
    let y = matrix_row Y i;
    backward 0.01 MLP cache y;
    match (epoch % 10) with
    | 0 -> (
      print `epoch {epoch} out loss {mse_loss out y}\n`;
      print_matrix row;
      print_matrix out;
      ()
    )
    | _ -> ()
  )
);

let input_test = fn () ->
  let input = Matrix 4 2 [|
    0., 0.,
    0., 1.,
    1., 0.,
    1., 1.
  |];

  for i = 0 to input.rows in (
    let row = matrix_row input i;
    let out = row |> infer MLP;
    print `TEST 1x2:\n`;
    print_matrix row;
    print_matrix out
  )
;;

input_test ()

